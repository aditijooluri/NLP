{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_question 8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditijooluri/NLP/blob/main/NLP_question_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Find BoW for the given paragraph? And also find stem and lemma words?"
      ],
      "metadata": {
        "id": "V9FHLWqQjdIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "fX5udRXJp5DX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([paragraph])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['WORD','FREQUENCY']\n",
        "print(count_occur_df)"
      ],
      "metadata": {
        "id": "bZ93vVD25fUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af1609c-678c-48f0-e4a6-08cb02fae2ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             WORD  FREQUENCY\n",
            "0         already          1\n",
            "1             and          1\n",
            "2    applications          1\n",
            "3             are          1\n",
            "4        articles          1\n",
            "5           books          1\n",
            "6           bound          1\n",
            "7          decide          1\n",
            "8         digital          1\n",
            "9       documents          1\n",
            "10         entire          1\n",
            "11           ever          1\n",
            "12             go          1\n",
            "13        growing          2\n",
            "14            has          1\n",
            "15           have          1\n",
            "16           here          1\n",
            "17           huge          1\n",
            "18         impact          1\n",
            "19             is          3\n",
            "20       language          1\n",
            "21          lives          1\n",
            "22          media          1\n",
            "23        natural          1\n",
            "24            nlp          1\n",
            "25            not          1\n",
            "26             of          2\n",
            "27             on          1\n",
            "28            one          1\n",
            "29             or          1\n",
            "30            our          1\n",
            "31     processing          1\n",
            "32     publishing          1\n",
            "33  summarization          1\n",
            "34     technology          1\n",
            "35           text          1\n",
            "36     thankfully          1\n",
            "37            the          1\n",
            "38           they          1\n",
            "39           this          1\n",
            "40          those          1\n",
            "41        through          1\n",
            "42           time          1\n",
            "43             to          3\n",
            "44         useful          1\n",
            "45        whether          1\n",
            "46          which          1\n",
            "47            who          1\n",
            "48           with          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "Ux_lpyp1rPr-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "GHT_hhw5sJ35"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem words in paragraph\n",
        "text = word_tokenize(paragraph)\n",
        "for word in text:\n",
        "  stemword = stemmer.stem(word)\n",
        "  print(stemword)"
      ],
      "metadata": {
        "id": "XZ_5WDDKD9G1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2304fc9-88e8-40bd-f597-c7d59d6f6424"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text\n",
            "summar\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "applic\n",
            "of\n",
            "natur\n",
            "languag\n",
            "process\n",
            "(\n",
            "nlp\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "live\n",
            ".\n",
            "with\n",
            "grow\n",
            "digit\n",
            "media\n",
            "and\n",
            "ever-grow\n",
            "publish\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entir\n",
            "articl\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decid\n",
            "whether\n",
            "they\n",
            "are\n",
            "use\n",
            "or\n",
            "not\n",
            "?\n",
            "thank\n",
            "–\n",
            "thi\n",
            "technolog\n",
            "is\n",
            "alreadi\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma words in paragraph\n",
        "for word in text:\n",
        "  w = Word(word)\n",
        "  lemmaword = w.lemmatize()\n",
        "  print(lemmaword)"
      ],
      "metadata": {
        "id": "TRFIOhZDFmAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d492e7-c228-4392-8c4d-33a22a80908b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text\n",
            "Summarization\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "application\n",
            "of\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "life\n",
            ".\n",
            "With\n",
            "growing\n",
            "digital\n",
            "medium\n",
            "and\n",
            "ever-growing\n",
            "publishing\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entire\n",
            "article\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decide\n",
            "whether\n",
            "they\n",
            "are\n",
            "useful\n",
            "or\n",
            "not\n",
            "?\n",
            "Thankfully\n",
            "–\n",
            "this\n",
            "technology\n",
            "is\n",
            "already\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    }
  ]
}